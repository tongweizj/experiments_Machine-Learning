{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe12b03",
   "metadata": {},
   "source": [
    "# Pretrained Models\n",
    "\n",
    "In this section we will talk about using a pretrained CNN as apart of our own custom network to improve the accuracy of our model.\n",
    "\n",
    "We know that CNN's alone (with no dense layers) don't do anything other than map the presence of features from our input. \n",
    "\n",
    "This means we can use a pretrained CNN, one trained on millions of images, as the start of our model. \n",
    "This will allow us to have a very good convolutional base before adding our own dense layered classifier at the end. \n",
    "In fact, by using this techique we can train a very good classifier for a realtively small dataset (< 10,000 images). \n",
    "\n",
    "This is because the convnet already has a very good idea of what features to look for in an image and can find them very effectively. So, if we can determine the presence of features all the rest of the model needs to do is determine which combination of features makes a specific image.\n",
    "\n",
    "## Fine Tuning\n",
    "\n",
    "When we employ the technique defined above, we will often want to tweak the final layers in our convolutional base to work better for our specific problem.\n",
    "\n",
    "This involves not touching or retraining the earlier layers in our convolutional base but only adjusting the final few. \n",
    "We do this because the first layers in our base are very good at extracting low level features lile lines and edges, things that are similar for any kind of image. \n",
    "Where the later layers are better at picking up very specific features like shapes or even eyes. If we adjust the final layers than we can look for only features relevant to our very specific problem.\n",
    "\n",
    "微调\n",
    "当我们使用上面定义的技术时，我们通常会希望调整卷积基础的最终层，以便更好地解决我们的具体问题。\n",
    "\n",
    "这涉及不修改或重新训练卷积基础中较早的层，而只调整最后几层。\n",
    "我们这样做是因为基础中的前几层非常擅长提取低级特征，例如线条和边缘，这些特征对于任何类型的图像来说都是相似的。\n",
    "而后面的层更擅长提取非常具体的特征，例如形状甚至眼睛。如果我们调整最终层，那么我们就可以只寻找与我们特定问题相关的特征。\n",
    "\n",
    "## Using a Pretrained Model\n",
    "In this section we will combine the tecniques we learned above and use a pretrained model and fine tuning to classify images of dogs and cats using a small dataset.\n",
    "\n",
    "*This tutorial is based on the following guide from the TensorFlow documentation*:\n",
    "- https://www.tensorflow.org/tutorials/images/transfer_learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c273177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 23:15:09.611091: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-04 23:15:09.611133: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-04 23:15:09.611815: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-04 23:15:09.616001: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-04 23:15:10.162467: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "keras = tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cccc32f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtfds\u001b[39;00m\n\u001b[32m      2\u001b[39m tfds.disable_progress_bar()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# split the data manually into 80% training, 10% testing, 10% validation\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "# split the data manually into 80% training, 10% testing, 10% validation\n",
    "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
    "    'cats_vs_dogs',\n",
    "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd0d2a2",
   "metadata": {},
   "source": [
    "# creates a function object that we can use to get labels\n",
    "get_label_name = metadata.features['label'].int2str  \n",
    "\n",
    "# display 2 images from the dataset\n",
    "for image, label in raw_train.take(5):\n",
    "  plt.figure()\n",
    "  plt.imshow(image)\n",
    "  plt.title(get_label_name(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d75019",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 160 # All images will be resized to 160x160\n",
    "\n",
    "def format_example(image, label):\n",
    "  \"\"\"\n",
    "  returns an image that is reshaped to IMG_SIZE\n",
    "  \"\"\"\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image = (image/127.5) - 1\n",
    "  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "  return image, label\n",
    "\n",
    "train = raw_train.map(format_example)\n",
    "validation = raw_validation.map(format_example)\n",
    "test = raw_test.map(format_example)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "\n",
    "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "validation_batches = validation.batch(BATCH_SIZE)\n",
    "test_batches = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657cfc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, label in raw_train.take(2):\n",
    "  print(\"Original shape:\", img.shape)\n",
    "\n",
    "for img, label in train.take(2):\n",
    "  print(\"New shape:\", img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c5e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c616db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, _ in train_batches.take(1):\n",
    "   pass\n",
    "\n",
    "feature_batch = base_model(image)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac25241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "base_model.summary()\n",
    "\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = keras.layers.Dense(1)\n",
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "  prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a94983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "  prediction_layer\n",
    "])\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# We can evaluate the model right now to see how it does before training it on our new images\n",
    "initial_epochs = 3\n",
    "validation_steps=20\n",
    "\n",
    "loss0,accuracy0 = model.evaluate(validation_batches, steps = validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ee774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can train it on our images\n",
    "history = model.fit(train_batches,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_batches)\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3d-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
